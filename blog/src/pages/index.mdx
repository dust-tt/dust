import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

![](@/media/2023-06-02-speculative-sampling/example.gif)

## Speculative sampling: LLMs writing a lot faster using other LLMs {{ date: '2023-06-02T00:00Z', id: '2023-06-02-speculative-sampling' }}

Behind the beautiful name of _Speculative sampling_ lies a neat technique to have a large language model can generate tokens **up to three times faster** üî• The technique has been developed by various research teams, including one from Google DeepMind [who published it here](https://arxiv.org/pdf/2302.01318.pdf).

---

![](@/media/2023-05-15-product-constitution/img.png)

## A first version of our Product Constitution  {{ date: '2023-05-15T00:00Z', id: '2023-05-15-product-constitution' }}

The opportunity opened by Large Language Models (LLMs) is vast. It‚Äôs allowed the birth of Dust, an AI-native company that wants to make work ‚Äúwork better‚Äù for smart teams. Deciding on a product constitution to guide what Dust would focus on in the vast space of possible directions was important to us. It will will help us decide locally more effectively. We look forward to continuously improving this first version we‚Äôre sharing here.

