export const description = "An overview of Dust's core blocks.";

# Core Blocks

An overview of Dust's core blocks. You'll learn about their functionality and usage. {{ className:
'lead' }}

Dust apps are composed of [Blocks](/overview#blocks) which are executed sequentially. Each block
produces outputs and can refer to the outputs of previously executed blocks. We invite you to review
the sections on [Inputs](/overview#inputs) and [Execution](/overview#execution) to better
understand the execution logic of a Dust app.

For each block we describe its _specification_ parameters and _configuration_ parameters. Please
refer to the [Blocks](/overview#blocks) section for more details on the difference between the two.

---

## Input block

The `input` block is the entry point to a Dust app and receives the arguments on which the app is
executed, in the form of JSON objects. During the design phase of the app it is associated to a
[Dataset](/overview#datasets) of examples used to iterate on the app. When deployed by API, the
`input` block receives the arguments passed to the app by API.

Refer to the [Inputs](/overview#inputs) section for more details on how the `input` block forks the
execution of the app in independent execution streams. When run by API, the dataset to which the
`input` block is associated is ignored and replaced by the arguments passed by API.

### Specification

<Properties>
  <Property name="dataset" type="dataset">
    The dataset of examples to be executed by the app during the design phase.
    Ignored and replaced by the arguments passed when run by API.
  </Property>
</Properties>

---

## Data block

The `data` block outputs the dataset it is associated with. Unlike the `input` block it does not
fork the execution of the app and simply returns the entire dataset as an array of JSON objects.

The `data` block is generally used in conjunction with an `llm` block to output a dataset of
few-shot examples to prompt models.

### Specification

<Properties>
  <Property name="dataset" type="dataset">
    The dataset of examples to be returned by the block as array.
  </Property>
</Properties>

---

## Code block

The `code` block executes the Javascript code provided by the user. The code must define a function
`_fun` which takes as input an `env` variable. `_fun` is executed as the block is run by the app.
`code` blocks are generally used to glue other blocks together by postprocessing previous blocks
outputs.

### Specification

<Properties>
  <Property name="code" type="javascript">
    The code to be executed. Exposed as a function named `_fun` taking an `env`
    variable as argument.
  </Property>
</Properties>

### Properties of the `env` variable

<Properties>
  <Property name="state" type="object">
    An object whose fields are the name of previously executed blocks and values
    are the output of the associated blocks. The output of a previously executed
    `EXAMPLE` block can be accessed as `env.state.EXAMPLE`.
  </Property>
  <Property name="input" type="object">
    An object with fields `input` and `index`. `input` is the input object of
    the current execution stream, _null_ if there is no `input` block executed
    yet. `index` is the index of the current input object in the dataset
    associated with the `input` block.
  </Property>
  <Property name="map" type="optional object">
    An optional object set only in the context of a block being executed as part
    of a `map` `reduce` pair. If set, contains a field `name` set to the name of
    the current `map` block and a field `iteration` which is the index of the
    element being executed as part of the `map` `reduce`.
  </Property>
  <Property name="config" type="object">
    The configuration with which the app is currently run. An object whose keys
    are block names and values are the associated configuration values.
  </Property>
</Properties>

---

## LLM block

The `llm` block provides a standardized interface to large language models from multiple providers
(OpenAI, Cohere, AI21, ...). It also provides automatic caching mechanisms and a rich templating
language (based on [Tera](https://tera.netlify.app/docs/#templates), similar go Jinja2) to construct
prompts.

It is used to issue completion requests to large language models as part of an app execution.

### Specification

<Properties>
  <Property name="prompt" type="string">
    The prompt to generate completions. The prompt can be templated using the
    [Tera](https://tera.netlify.app/docs/#templates) templating language (see
    below for more details on templating).
  </Property>
  <Property name="temperature" type="float">
    The temperature to use when sampling from the model. A higher _temperature_
    (eg _1.0_) results in more diverse completions, while a lower _temperature_
    (eg _0.0_) results in more conservative completions.
  </Property>
  <Property name="max_tokens" type="integer">
    The maximum number of tokens to generate from the model. It can be very
    broadly seen as a word worth of content. The model may decide to stop
    generation before reaching `max_tokens`. For OpenAI models, we support _-1_
    as a special value, specifying that we want the model to generate as many
    tokens as possible given its context size. Using _-1_ with a model of
    context size 2048 with a prompt consuming 1024 tokens, is equivalent to
    specifying a `max_tokens` of _1024_.
  </Property>
  <Property name="stop" type="[]string">
    An array of strings that should interrupt completion when sampled from the
    model.
  </Property>
  <Property name="frequency_penalty" type="float">
    Number between _-2.0_ and _2.0_. Positive values penalize new tokens based
    on their existing frequency in the text so far, decreasing the model's
    likelihood to repeat the same line verbatim.
  </Property>
  <Property name="presence_penalty" type="float">
    Number between _-2.0_ and _2.0_. Positive values penalize new tokens based
    on whether they appear in the text so far, increasing the model's likelihood
    to talk about new topics.
  </Property>
  <Property name="top_p" type="float">
    An alternative to sampling with temperature, called nucleus sampling, where
    the model considers the results of the tokens with `top_p` probability mass.
    So _0.1_ means only the tokens comprising the top 10% probability mass are
    considered.
  </Property>
  <Property name="top_logprobs" type="integer">
    Include the log probabilities on the `top_logprobs` most likely tokens, as
    well as the chosen token. For example, if logprobs is _5_, the API will
    return a list of the 5 most likely tokens' probabilities. Useful for
    classification tasks.
  </Property>
</Properties>

### Configuration

<Properties>
  <Property name="provider_id" type="string">
    The model provider to use. One of _openai_, _cohere_, _ai21_.
  </Property>
  <Property name="model_id" type="string">
    The model to use from the provider specified by `provider_id`.
  </Property>
  <Property name="use_cache" type="bool">
    Whether to rely on the automated caching mechanism of the `llm` block. If
    set to _true_ and a previous request was made with the same specification
    and configuration parameters, then the cached response is returned.
  </Property>
  <Property name="use_stream" type="bool">
    In the context of running an app by API. Whether to stream each token as
    they are emitted by the model. Currently only supported for `provider_id`
    set to `openai`.
  </Property>
</Properties>

### Prompt templating

The `prompt` field of the `llm` block can be templated using the
[Tera](https://tera.netlify.app/docs/#templates). This is particularly useful to construct few-shot
prompts for models. Here's an example of templated model prompt:

```
{% for e in EXAMPLES %}
EN: {{e.english}}
FR: {{e.french}}
{% endfor %}
EN: {{INPUT.english}}
FR:
```

In this example, for each object in the output of the `EXAMPLES` block (which is an array of JSON
objects), create a line with the English sentence and French translation. Then, add a final line
with the English sentence from the `INPUT` block output `english` field. A fully functional example
app relying on this prompt can be found [here](https://dust.tt/w/3e26b0e764/a/d903a92151).

The [Tera](https://tera.netlify.app/) templating language supports a variety of constructs including
loops and variable replacement. Please refer to the [Tera
documentation](https://tera.netlify.app/docs/#templates) for more details.

### Support for chat-based models (chatGPT)

We also support chat-based models such as OpenAI's `gpt-3.5-turbo` ([Chat
API](https://platform.openai.com/docs/guides/chat)) and `gpt-4`. When these models are used, as part
of the `llm` block, `max_tokens` and `top_logprobs` are ignored, and the content of the templated
prompt is passed as initial message with _user_ role. The block response is the content of the
`assistant` role message returned by the model. Note that logprobs and tokens are not available with
these models.

---

## Chat block

The `chat` block provides a standardized interface to chat-based large language models such as
OpenAI's `gpt-3.5-turbo` ([Chat API](https://platform.openai.com/docs/guides/chat)) and `gpt-4`. It
is similar in many respects to an LLM block, it has templated (see LLM block above) intstructions
that serve as initial prompt and exposes a `messages` code block to output previous messages
(generally passed to the app as input)

It is used to build chat-based experiences where the model is exposed with previous interactions and
generates a new message in response.

### Specification

<Properties>
  <Property name="instructions" type="string">
    The instructions are passed to the model as initial _system_ role message
    (see OpenAI's [Chat guide](https://platform.openai.com/docs/guides/chat)).
    The instructions can be templated using the
    [Tera](https://tera.netlify.app/docs/#templates) templating language (see
    above for more details on templating).
  </Property>
  <Property name="messages_code" type="javascript">
    A Javascript function `_fun` which takes a single argument `env` (see the
    [Code block](/core-blocks#code-block) documentation for details) and returns
    an array (possibly empty) of messages (but you generally want at least one
    _user_ role message). Messages should be objects with two fields: _role_ and
    _content_. Values should be string. Possible values for `role` are _user_,
    _assistant_ and _system_ (see OpenAI's [Chat
    guide](https://platform.openai.com/docs/guides/chat)).
  </Property>
  <Property name="temperature" type="float">
    The temperature to use when sampling from the model. A higher _temperature_
    (eg _1.0_) results in more diverse completions, while a lower _temperature_
    (eg _0.0_) results in more conservative completions.
  </Property>
  <Property name="stop" type="[]string">
    An array of strings that should interrupt completion when sampled from the
    model.
  </Property>
  <Property name="max_tokens" type="integer">
    The maximum number of tokens to generate from the model for the next
    message. It can be very broadly seen as a word worth of content. The model
    may decide to stop generation before reaching `max_tokens`.
  </Property>
  <Property name="frequency_penalty" type="float">
    Number between _-2.0_ and _2.0_. Positive values penalize new tokens based
    on their existing frequency in the text so far, decreasing the model's
    likelihood to repeat the same line verbatim.
  </Property>
  <Property name="presence_penalty" type="float">
    Number between _-2.0_ and _2.0_. Positive values penalize new tokens based
    on whether they appear in the text so far, increasing the model's likelihood
    to talk about new topics.
  </Property>
  <Property name="top_p" type="float">
    An alternative to sampling with temperature, called nucleus sampling, where
    the model considers the results of the tokens with `top_p` probability mass.
    So _0.1_ means only the tokens comprising the top 10% probability mass are
    considered.
  </Property>
</Properties>

### Configuration

<Properties>
  <Property name="provider_id" type="string">
    The model provider to use. One of _openai_, _cohere_, _ai21_.
  </Property>
  <Property name="model_id" type="string">
    The model to use from the provider specified by `provider_id`.
  </Property>
  <Property name="use_cache" type="bool">
    Whether to rely on the automated caching mechanism of the `llm` block. If
    set to _true_ and a previous request was made with the same specification
    and configuration parameters, then the cached response is returned.
  </Property>
  <Property name="use_stream" type="bool">
    In the context of running an app by API. Whether to stream each token as
    they are emitted by the model. Currently only supported for `provider_id`
    set to `openai`.
  </Property>
</Properties>

---

## Map Reduce blocks

The `map` and `reduce` blocks are used to execute the set of blocks in between them on each elements
of an array, in parallel. The `map` block takes a block name as `from` specification argument. If
the output of the block referred to is an array, the execution stream will fork on each element of
the array. If the output is an object you can use the `repeat` specification argument to map on the
same element `repeat` times.

The `reduce` block does not take any argument and has the same name as its associated `map` block.
After executing a `reduce` block, each output of the blocks in between the `map` and `reduce` blocks
will be collected in an array accessible from any subsequent block.

Assume we have a **MAPREDUCE** `map` block whose `from` specification argument points to a block
whose output is an array of length 4. Assume it is followed by a **DUMMY** `code` block and an
associated **MAPREDUCE** `reduce` block, and finally a **FINAL** code block. Also assume that the
output of the **DUMMY** block is a simple `{ "foo": "bar" }` object. The **DUMMY** code block will
be executed 4 times in parallel, and as we execute the **FINAL** block, `env.state.DUMMY` will
contain an array of 4 `{ "foo": "bar" }` objects.

### Configuration

<Properties>
  <Property name="from" type="string">
    The name of the block's output to map on. If the output is an array, the
    execution stream will fork on each element of the array. If the output is an
    object you the `repeat` specification argument must be specified.
  </Property>
  <Property name="repeat" type="integer">
    Only valid if the output of the `from` block is an object. The number of
    times to repeat the execution of the blocks in between the `map` and
    `reduce` blocks on the same object.
  </Property>
</Properties>

---

## DataSource block

The `data_source` block provides an interface to query a [data source](/data-sources-overview) and
return chunks that are semantically similar to the query provided. When executed, the query is
embedded using the embedding model set on the data source and the resulting enmbedding vector
is used to perform semantic search on the data source's chunks.

The output of the `data_source` block is a list of [Document](/documents#the-document-model)
objects. Each document may include one or more [Chunks](/documents#the-chunk-model) (only those
that were returned from the search). The documents are sorted by decreasing order of the max of
their retrieved chunks score. Please refer to the [data sources overview](/data-sources-overview)
for more details about how documents are chunked to enable semantic search.

### Specification

<Properties>
  <Property name="query" type="string">
    The query to embed and use to perform semantic seach against the data
    source. The query can be templated using the
    [Tera](https://tera.netlify.app/docs/#templates) templating language (see
    the [LLM block](/core-blocks#llm-block) documentation for more details on
    templating).
  </Property>
  <Property name="full_text" type="boolean">
    Whether to return the full text of the retrieved documents (in addition to
    each chunk text). If true, each returned
    [Document](/documents#the-document-model) object will have a `text` property
    containing the full text of the document.
  </Property>
</Properties>

### Configuration

<Properties>
  <Property name="top_k" type="integer">
    The number of chunks to return. The resulting number of document objects may
    be smaller if multiple chunks from the same document are among the `top_k`
    retrieved chunks.
  </Property>
  <Property name="data_sources" type="[]{username, data_source_id}">
    An array of objects representing the data sources to query. Note that the
    Dust interface currently only supports adding one data source to a block,
    but you can pass many by API. The objects must have the following
    properties: `username`, the username of the user who owns the data source,
    and, `data_source_id`, The name of the data source.
  </Property>
  <Property
    name="filter"
    type="optional {tags: {in, not}, timestamp: {gt, lt}}"
  >
    An object representing the filters to apply to the query. The `tags` field
    is an object with properties `in` and `not`, both arrays of strings (the
    tags to filter on). The documents' tags must match at least one of the tags
    specified in `in` and none of the ones specified in `not`. The `timestamp`
    field is an object with properties `lg` and `gt`. The query will only return
    chunks from documents whose timestamp is greater than `gt` and lower than
    `lt`. The timestamp values are represented as epoch in ms. The filter
    configuration is optional. And each of the fields in `tags` or `timestamp`
    are also optional. Example filter:
    ```json
    {
      "tags": {"in": ["tag1", "tag2"], "not": null},
      "timestamp": {"gt": 1675215950729, "lt": 1680012404017}
    }
    ```
  </Property>
</Properties>
