export const description =
  'An introduction to large language models, prompting, and large language model apps design.'

# Introduction

A introduction to large language models, prompting and large language model apps design, covering,
with no prerequesites, the core concepts required to build applications on top of large language
models. {{ className: 'lead' }}

## Large Language Models

A large language model is a deep-learning system (generally based on the [Transformer](#)
architecture) capable of generating text. Large language models operate on **tokens** which
represent words or parts of a word (tokenization is a bijection between text and lists of tokens
required by the transformer architecture). They are trained over internet-scale datasets
([CommonCrawl](#), [StackOverflow](#), [Github](#), ...) to predict the next **token** given a
list of tokens passed as input.

<Note>
  Given some text as input, the **prompt**, a large language model will generate
  a likely completion based on its training data. It can simply be seen as a
  massive auto-complete system trained at Internet-scale.
</Note>

To generate text from a large language model, we use what is refered to as _auto-regressive
decoding_. Given a suite of tokens as input, generally called a **prompt**, we use the model to
predict the next token. Once this token is predicted we append it to the current prompt and
reiterate the same process until the model emits a special `<|EndofText|>` token or runs out of
context.

### Context Size

To predict the next token, Transformer-based large language models will _attend_ (look at all
previous tokens, the prompt) with a processus called _Attention_. This computationally expensive
process imposes constraints on the maximum amount of text a model can operate with. This maximum
length is referred to as its **context size**. Each large language model has a specific
context-size, but it generally consists of 4000 tokens (~4000 words) (some have 2000, some others
have up to 8000).

When working with large language model, context size is one of the biggest limiting factor, as it
bounds the amount of information you can use to condition the prediction of the model.

### Prompting

If you want to get a model to continue an article, you'll generally use the beginning of the article
as prompt, providing amply enough signal for the model to _understand_ that you're interested in
generating the rest of the article. But let say you want to translate a piece of english text to
french? How would encode to the model the fact that you want your sentence translated?

For models that are solely trained on data extracted from the Internet, the best technique to encode
which task you're interested in is to use **few-shot prompting**. Few-shot prompting consists in
demonstrating to the model a few examples of the task you're interested in as a way to _encode_ that
task. As an example, the following prompt will successfully lead to your `<YOUR SENTENCE>` being
translated in french:

```
EN: Good Morning.
FR: Bonjour.

EN: The car is blue.
FR: La voiture est bleue.

EN: &lt;YOUR SENTENCE&gt;
FR:
```

Note that the examples we provide to the model do not serve as training data (translation being an
emerging capability of large language models that is acquired during training), but instead solely
serve as a way to encode the task we care about. We say that large language models are **few-shot
learners**; they are capable of understanding a task with just a few examples.

<Note>
  Few-shot prompting, consisting in presenting a few examples of a task, is an
  effective way to encode to models the task you care about.
</Note>

Intuitively, if you had read and somewhat memorized the entire Internet, and were tasked to generate
the most likely completion to the prompt above, similarly to the model you would likely decide that
the correct continuation is a translation of the last sentence. Large language models are
basically the same. And in many aspects, being trained on human-generated data, they often appear as
surprisingly _anthropomorphic_.

### Instruction-following

Now assume you would like to summarize a somewhat long piece of text. You are faced with a new
problem, as providing a few examples of the task might consume too much of your limited context
size. Instead you could try to provide _instructions_ to the model:

```
&lt;LONG TEXT&gt;
Generate a 1-paragraph summary of the text above:
```

While this is a good idea, for models that were solely trained on data from the Internet, this
approach is in practice quite brittle compared to few-shot prompting. Not that often do documents
appear on the Internet with instructions, followed by the result of applying these instructions. Not
impossible but not common, hence the reason why it works at times but remains brittle. A naive use
of instructions on such models will often lead to the model ignoring instructions (maybe assuming it
was text from an ad) or failing to properly apply them.

<Note>
  Instruction-following models alleviates the need for few-shot prompting.
</Note>

To solve this issue, large language models can be **fine-tuned** (trained for a bit longer on top of
their initial training) for instruction-following. Various techniques exist for this process, but
the important part is that the resulting model is biased towards following instructions, rendering
viable the approach described above.

Along with a well thought-out UI, improvements in instruction-following was one of the core advances
of ChatGPT, possibly explaining its rapid success. ChatGPT relied on a novel instruction-following
fine-tuning paradigm called [reinforcement learning from human feedback](#).

### Fine-tuning

Another way to encode a task, as well as teach the model new information is to use fine-tuning.
Since it involves training, fine-tuning will drastically alter the _distribution_ (the behavior) of
your model.

Let say you have a database of hundreds of thousands of customer support interactions in the form of
`(question, answer)` pairs. To construct a customer-support bot, you could fine-tune a model on the
following distribution (or dataset, the two can be used interchangeably as a dataset represents a
distribution):

```
Q: &lt;QUESTION&gt;
A: &lt;ANSWER&gt;
&lt;|EndOfText|&gt;
```

Anytime a question would need answering, you could provide the following prompt to the fine-tuned
model:

```
Q: &lt;QUESTION&gt;
A:
```

... and expect to get a useful answer. Given that this dataset contains a lot of information about
your company or products, in order to be able to accurately predict each token (which is the
fine-tuning or training objective), the model would have to memoize that information. Additionally,
the task would be encoded in the new distribution of the model, and instruction-following or
few-show prompting would not be required anymore.

**Caveats.** This looks ideal but there are a few caveats one has to be careful about with
_fine-tuning_. First given the size of typical models, a lot of data is needed for fine-tuning to
work well in practice (think 10m-100m words). Additionally, it is unclear (it is still an open
research question) how much fine-tuning brain-damages the model (rendering it less smart). Finally
one has to be careful about exactly how they encode the task. As an example, if the date of the
question is not shown to the model in the examples above, the model might consider deprecated
information as still relevant and answer erroneously, or maybe learn spurious statistical
relationships between the type of question and the tone of the answer.

Fine-tuning is an incredibly powerful mechanism. But it is important to remember that it entangles
the process of encoding a task and teaching new information to the model. The latter possibly
leading to surprising or unexpected results.

## Large Language Model Apps

A large language model app is a chain of one or multiple pompted calls to models or external
services (such as APIs or data sources) in order to achieve a particular task. Said otherwise, a
large language model app is an orchestration layer that sits on top of a model in order to
specialize its behavior to perform a specific task.

<Note>
  A large language model app is a chain of one or multiple pompted calls to
  models or external services (such as APIs or data sources) in order to achieve
  a particular task.
</Note>

As an example, the translation prompt above which consists in **data** (the few-shot examples), a
**template** (the way these examples are presented to the model), reified as a function that, given a
sentence, returns the output of the model, can be seen as a basic large language model app.

// TODO(spolu): Screenshot of Dust LLM block



The design of a large language model app is the process of defining prompts and chains of large
language models or external services calls to expose a structured API with a predictable behavior
built on top of large language models' generic interface. The purpose of Dust is to help design and
deploy such large language model apps.

In the rest of this document, we'll present three examples of large language model apps of
increasing complexity with links to their implementation in Dust.

### (Easy) Translation

As we have seen above, translation is a simple task that can be encoded using a few-shot prompt. A
large language model app for translation therefore mostly consists in one parametrized call to a
large language model. In Dust, this is achievable with the following blocks (most of these blocks
are described in greater details the following pages):

- An `input` block to receive the text to translate (see [Inputs](/overview#inputs)).
- A `data` block that returns the few-shot examples maintained in a dataset (see
  [Datasets](/overview#datasets)).
- An `llm` block that parametrizes the call to the large language model (see [TODO(spolu)](#)).
- A `code` block to process the output of the large language model (see [TODO(spolu)](#)).

The `llm` block main parameter is the prompt. The prompt is a templated string that will be
constructed from the `data` block and the `input` block. The template we use for this app is the
following:

```
{% for e in EXAMPLES %}
EN: {{e.english}}
FR: {{e.french}}
{% endfor %}
EN: {{INPUT.english}}
FR:
```

The templating language used by `llm` blocks is based on the [Tera](https://tera.netlify.app/)
engine, similar to Jinja2. The `llm` block will construct the prompt by replacing the
`{{e.english}}`, `{{e.french}}` and `{{INPUT.english}}` placeholders with the actual data returned
by the `data` block (called `EXAMPLES` here) and the `input` block (called `INPUT` here). Note the
use of a loop to iterate over the examples (lines 1 and 4).

You can find and explore the full implementation of this app in Dust here:
[intro-translation](https://dust.tt/spolu/a/d903a92151). In particular you can introspect the prompt
sent to the model and the completion it returned by clicking on the outputs of the `llm` block.

### (Medium) Recursive Summarization

Recursive summarization consists in summarizing a piece of text that is too large to hold in the
context of the large language model. The technique consists in splitting the text in chunks and
summarizing each chunk separately. The output of the summarization of each chunk is then
concatenated and used to produce a final summary.

If the text to summarize is extremely long, this process can be repeated recursively until the
concatenation of the chunks fit in-context. Hence the name *recursive summarization*.

To summarize each chunk as well as produce the final summary we can use a model fine-tuned for
instruction-following such as OpenAI's `text-davinci-003`.

For each chunk we can use the following prompt to obtain a summary:

```
{{CHUNK}}

Summarize the paragraphs above in under 512 characters.

SUMMARY:
```

To obtain the final summary the following prompt has been demonstrated to work well:

```
TITLE: {{INPUT.title}}
AUTHOR: {{INPUT.author}}
CONTENT: {{AGGREGATE.blob}}

Summarize the text above in one ~1000 characters paragraph, not repeating the title or author.

SUMMARY:
```

These two prompts are just examples of how to encode the task of summarization to an
instruct-following model. They can likely be improved or modified to produce specific results.

The Dust app that demonstrates this technique leverages blocks such as `map` and `reduce` that will
be more clearly explained in the following pages. Nonetheless you can find and explore its full
implementation here: [recusive-summarization](https://dust.tt/spolu/a/eeac37ce40).


### (Advanced) Mathematics
