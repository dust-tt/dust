{"code":"/**\n * Supported models\n */\nexport const GPT_4_MODEL_ID = \"gpt-4\";\nexport const GPT_4_TURBO_PREVIEW_MODEL_ID = \"gpt-4-turbo-preview\";\nexport const GPT_3_5_TURBO_MODEL_ID = \"gpt-3.5-turbo-1106\";\nconst GPT_4_DESCRIPTION = \"OpenAI's most powerful and recent model (128k context).\";\nconst GPT_4_SHORT_DESCRIPTION = \"OpenAI's smartest model.\";\nexport const GPT_4_MODEL_CONFIG = {\n    providerId: \"openai\",\n    modelId: GPT_4_MODEL_ID,\n    displayName: \"GPT 4\",\n    contextSize: 8192,\n    recommendedTopK: 16,\n    largeModel: true,\n    description: GPT_4_DESCRIPTION,\n    shortDescription: GPT_4_SHORT_DESCRIPTION,\n};\nexport const GPT_4_TURBO_MODEL_CONFIG = {\n    providerId: \"openai\",\n    modelId: GPT_4_TURBO_PREVIEW_MODEL_ID,\n    displayName: \"GPT 4\",\n    contextSize: 128000,\n    recommendedTopK: 32,\n    largeModel: true,\n    description: GPT_4_DESCRIPTION,\n    shortDescription: GPT_4_SHORT_DESCRIPTION,\n};\nexport const GPT_3_5_TURBO_MODEL_CONFIG = {\n    providerId: \"openai\",\n    modelId: GPT_3_5_TURBO_MODEL_ID,\n    displayName: \"GPT 3.5 Turbo\",\n    contextSize: 16384,\n    recommendedTopK: 16,\n    largeModel: false,\n    description: \"OpenAI's cost-effective and high throughput model (16k context).\",\n    shortDescription: \"OpenAI's fast model.\",\n};\nexport const CLAUDE_2_1_MODEL_ID = \"claude-2.1\";\nexport const CLAUDE_2_MODEL_ID = \"claude-2\";\nexport const CLAUDE_INSTANT_1_2_MODEL_ID = \"claude-instant-1.2\";\nexport const CLAUDE_DEFAULT_MODEL_CONFIG = {\n    providerId: \"anthropic\",\n    modelId: CLAUDE_2_1_MODEL_ID,\n    displayName: \"Claude 2.1\",\n    contextSize: 200000,\n    recommendedTopK: 32,\n    largeModel: true,\n    description: \"Anthropic's superior performance model (200k context).\",\n    shortDescription: \"Anthropic's smartest model.\",\n};\nexport const CLAUDE_INSTANT_DEFAULT_MODEL_CONFIG = {\n    providerId: \"anthropic\",\n    modelId: CLAUDE_INSTANT_1_2_MODEL_ID,\n    displayName: \"Claude Instant 1.2\",\n    contextSize: 100000,\n    recommendedTopK: 32,\n    largeModel: false,\n    description: \"Anthropic's low-latency and high throughput model (100k context)\",\n    shortDescription: \"Anthropic's fast model.\",\n};\nexport const MISTRAL_LARGE_MODEL_ID = \"mistral-large-latest\";\nexport const MISTRAL_MEDIUM_MODEL_ID = \"mistral-medium\";\nexport const MISTRAL_NEXT_MODEL_ID = \"mistral-next\";\nexport const MISTRAL_SMALL_MODEL_ID = \"mistral-small\";\nexport const MISTRAL_LARGE_MODEL_CONFIG = {\n    providerId: \"mistral\",\n    modelId: MISTRAL_LARGE_MODEL_ID,\n    displayName: \"Mistral Large\",\n    contextSize: 31500,\n    recommendedTopK: 16,\n    largeModel: true,\n    description: \"Mistral latest `large` model (32k context).\",\n    shortDescription: \"Mistral's large model.\",\n};\nexport const MISTRAL_NEXT_MODEL_CONFIG = {\n    providerId: \"mistral\",\n    modelId: MISTRAL_NEXT_MODEL_ID,\n    displayName: \"Mistral (next)\",\n    contextSize: 31500,\n    recommendedTopK: 16,\n    largeModel: true,\n    description: \"Mistral's next generation model (32k context).\",\n    shortDescription: \"Mistral's next model.\",\n};\nexport const MISTRAL_MEDIUM_MODEL_CONFIG = {\n    providerId: \"mistral\",\n    modelId: MISTRAL_MEDIUM_MODEL_ID,\n    displayName: \"Mistral Medium\",\n    contextSize: 31500,\n    recommendedTopK: 16,\n    largeModel: true,\n    description: \"Mistral latest `medium` model (32k context).\",\n    shortDescription: \"Mistral's smartest model.\",\n};\nexport const MISTRAL_SMALL_MODEL_CONFIG = {\n    providerId: \"mistral\",\n    modelId: MISTRAL_SMALL_MODEL_ID,\n    displayName: \"Mistral Small\",\n    contextSize: 31500,\n    recommendedTopK: 16,\n    largeModel: false,\n    description: \"Mistral latest model (8x7B Instruct, 32k context).\",\n    shortDescription: \"Mistral's fast model.\",\n};\nexport const GEMINI_PRO_DEFAULT_MODEL_CONFIG = {\n    providerId: \"google_vertex_ai\",\n    modelId: \"gemini-pro\",\n    displayName: \"Gemini Pro\",\n    contextSize: 8192,\n    recommendedTopK: 16,\n    largeModel: true,\n    description: \"Google's best model for scaling across a wide range of tasks (8k context).\",\n    shortDescription: \"Google's smartest model.\",\n};\nexport const SUPPORTED_MODEL_CONFIGS = [\n    GPT_3_5_TURBO_MODEL_CONFIG,\n    GPT_4_MODEL_CONFIG,\n    GPT_4_TURBO_MODEL_CONFIG,\n    CLAUDE_DEFAULT_MODEL_CONFIG,\n    CLAUDE_INSTANT_DEFAULT_MODEL_CONFIG,\n    MISTRAL_LARGE_MODEL_CONFIG,\n    MISTRAL_MEDIUM_MODEL_CONFIG,\n    MISTRAL_NEXT_MODEL_CONFIG,\n    MISTRAL_SMALL_MODEL_CONFIG,\n    GEMINI_PRO_DEFAULT_MODEL_CONFIG,\n];\nexport function isSupportedModel(model) {\n    const maybeSupportedModel = model;\n    return SUPPORTED_MODEL_CONFIGS.some((m) => m.modelId === maybeSupportedModel.modelId &&\n        m.providerId === maybeSupportedModel.providerId);\n}\n//# sourceMappingURL=assistant.js.map","references":["/Users/edouardwautier/Projects/dust/types/src/shared/typescipt_utils.ts"],"map":"{\"version\":3,\"file\":\"assistant.js\",\"sourceRoot\":\"\",\"sources\":[\"../../../../../../src/front/lib/assistant.ts\"],\"names\":[],\"mappings\":\"AAAA;;GAEG;AAIH,MAAM,CAAC,MAAM,cAAc,GAAG,OAAgB,CAAC;AAC/C,MAAM,CAAC,MAAM,4BAA4B,GAAG,qBAA8B,CAAC;AAC3E,MAAM,CAAC,MAAM,sBAAsB,GAAG,oBAA6B,CAAC;AAEpE,MAAM,iBAAiB,GACrB,yDAAyD,CAAC;AAC5D,MAAM,uBAAuB,GAAG,0BAA0B,CAAC;AAE3D,MAAM,CAAC,MAAM,kBAAkB,GAAG;IAChC,UAAU,EAAE,QAAiB;IAC7B,OAAO,EAAE,cAAc;IACvB,WAAW,EAAE,OAAO;IACpB,WAAW,EAAE,IAAI;IACjB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,iBAAiB;IAC9B,gBAAgB,EAAE,uBAAuB;CAC1C,CAAC;AAEF,MAAM,CAAC,MAAM,wBAAwB,GAAG;IACtC,UAAU,EAAE,QAAiB;IAC7B,OAAO,EAAE,4BAA4B;IACrC,WAAW,EAAE,OAAO;IACpB,WAAW,EAAE,MAAM;IACnB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,iBAAiB;IAC9B,gBAAgB,EAAE,uBAAuB;CACjC,CAAC;AAEX,MAAM,CAAC,MAAM,0BAA0B,GAAG;IACxC,UAAU,EAAE,QAAiB;IAC7B,OAAO,EAAE,sBAAsB;IAC/B,WAAW,EAAE,eAAe;IAC5B,WAAW,EAAE,KAAK;IAClB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,KAAK;IACjB,WAAW,EACT,kEAAkE;IACpE,gBAAgB,EAAE,sBAAsB;CAChC,CAAC;AAEX,MAAM,CAAC,MAAM,mBAAmB,GAAG,YAAqB,CAAC;AACzD,MAAM,CAAC,MAAM,iBAAiB,GAAG,UAAmB,CAAC;AACrD,MAAM,CAAC,MAAM,2BAA2B,GAAG,oBAA6B,CAAC;AAEzE,MAAM,CAAC,MAAM,2BAA2B,GAAG;IACzC,UAAU,EAAE,WAAoB;IAChC,OAAO,EAAE,mBAAmB;IAC5B,WAAW,EAAE,YAAY;IACzB,WAAW,EAAE,MAAM;IACnB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,wDAAwD;IACrE,gBAAgB,EAAE,6BAA6B;CACvC,CAAC;AAEX,MAAM,CAAC,MAAM,mCAAmC,GAAG;IACjD,UAAU,EAAE,WAAoB;IAChC,OAAO,EAAE,2BAA2B;IACpC,WAAW,EAAE,oBAAoB;IACjC,WAAW,EAAE,MAAM;IACnB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,KAAK;IACjB,WAAW,EACT,kEAAkE;IACpE,gBAAgB,EAAE,yBAAyB;CACnC,CAAC;AAEX,MAAM,CAAC,MAAM,sBAAsB,GAAG,sBAA+B,CAAC;AACtE,MAAM,CAAC,MAAM,uBAAuB,GAAG,gBAAyB,CAAC;AACjE,MAAM,CAAC,MAAM,qBAAqB,GAAG,cAAuB,CAAC;AAC7D,MAAM,CAAC,MAAM,sBAAsB,GAAG,eAAwB,CAAC;AAE/D,MAAM,CAAC,MAAM,0BAA0B,GAAG;IACxC,UAAU,EAAE,SAAkB;IAC9B,OAAO,EAAE,sBAAsB;IAC/B,WAAW,EAAE,eAAe;IAC5B,WAAW,EAAE,KAAK;IAClB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,6CAA6C;IAC1D,gBAAgB,EAAE,wBAAwB;CAClC,CAAC;AAEX,MAAM,CAAC,MAAM,yBAAyB,GAAG;IACvC,UAAU,EAAE,SAAkB;IAC9B,OAAO,EAAE,qBAAqB;IAC9B,WAAW,EAAE,gBAAgB;IAC7B,WAAW,EAAE,KAAK;IAClB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,gDAAgD;IAC7D,gBAAgB,EAAE,uBAAuB;CACjC,CAAC;AAEX,MAAM,CAAC,MAAM,2BAA2B,GAAG;IACzC,UAAU,EAAE,SAAkB;IAC9B,OAAO,EAAE,uBAAuB;IAChC,WAAW,EAAE,gBAAgB;IAC7B,WAAW,EAAE,KAAK;IAClB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EAAE,8CAA8C;IAC3D,gBAAgB,EAAE,2BAA2B;CACrC,CAAC;AAEX,MAAM,CAAC,MAAM,0BAA0B,GAAG;IACxC,UAAU,EAAE,SAAkB;IAC9B,OAAO,EAAE,sBAAsB;IAC/B,WAAW,EAAE,eAAe;IAC5B,WAAW,EAAE,KAAK;IAClB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,KAAK;IACjB,WAAW,EAAE,oDAAoD;IACjE,gBAAgB,EAAE,uBAAuB;CACjC,CAAC;AAEX,MAAM,CAAC,MAAM,+BAA+B,GAAG;IAC7C,UAAU,EAAE,kBAA2B;IACvC,OAAO,EAAE,YAAY;IACrB,WAAW,EAAE,YAAY;IACzB,WAAW,EAAE,IAAI;IACjB,eAAe,EAAE,EAAE;IACnB,UAAU,EAAE,IAAI;IAChB,WAAW,EACT,4EAA4E;IAC9E,gBAAgB,EAAE,0BAA0B;CACpC,CAAC;AAEX,MAAM,CAAC,MAAM,uBAAuB,GAAG;IACrC,0BAA0B;IAC1B,kBAAkB;IAClB,wBAAwB;IACxB,2BAA2B;IAC3B,mCAAmC;IACnC,0BAA0B;IAC1B,2BAA2B;IAC3B,yBAAyB;IACzB,0BAA0B;IAC1B,+BAA+B;CACvB,CAAC;AAWX,MAAM,UAAU,gBAAgB,CAAC,KAAc;IAC7C,MAAM,mBAAmB,GAAG,KAAuB,CAAC;IACpD,OAAO,uBAAuB,CAAC,IAAI,CACjC,CAAC,CAAC,EAAE,EAAE,CACJ,CAAC,CAAC,OAAO,KAAK,mBAAmB,CAAC,OAAO;QACzC,CAAC,CAAC,UAAU,KAAK,mBAAmB,CAAC,UAAU,CAClD,CAAC;AACJ,CAAC\"}","dtsmap":{"name":"/Users/edouardwautier/Projects/dust/types/node_modules/.cache/rollup-plugin-typescript2/placeholder/front/lib/assistant.d.ts.map","writeByteOrderMark":false,"text":"{\"version\":3,\"file\":\"assistant.d.ts\",\"sourceRoot\":\"\",\"sources\":[\"../../../../../../src/front/lib/assistant.ts\"],\"names\":[],\"mappings\":\"AAAA;;GAEG;AAEH,OAAO,EAAE,mBAAmB,EAAE,MAAM,8BAA8B,CAAC;AAEnE,eAAO,MAAM,cAAc,SAAmB,CAAC;AAC/C,eAAO,MAAM,4BAA4B,uBAAiC,CAAC;AAC3E,eAAO,MAAM,sBAAsB,sBAAgC,CAAC;AAMpE,eAAO,MAAM,kBAAkB;;;;;;;;;CAS9B,CAAC;AAEF,eAAO,MAAM,wBAAwB;;;;;;;;;CAS3B,CAAC;AAEX,eAAO,MAAM,0BAA0B;;;;;;;;;CAU7B,CAAC;AAEX,eAAO,MAAM,mBAAmB,cAAwB,CAAC;AACzD,eAAO,MAAM,iBAAiB,YAAsB,CAAC;AACrD,eAAO,MAAM,2BAA2B,sBAAgC,CAAC;AAEzE,eAAO,MAAM,2BAA2B;;;;;;;;;CAS9B,CAAC;AAEX,eAAO,MAAM,mCAAmC;;;;;;;;;CAUtC,CAAC;AAEX,eAAO,MAAM,sBAAsB,wBAAkC,CAAC;AACtE,eAAO,MAAM,uBAAuB,kBAA4B,CAAC;AACjE,eAAO,MAAM,qBAAqB,gBAA0B,CAAC;AAC7D,eAAO,MAAM,sBAAsB,iBAA2B,CAAC;AAE/D,eAAO,MAAM,0BAA0B;;;;;;;;;CAS7B,CAAC;AAEX,eAAO,MAAM,yBAAyB;;;;;;;;;CAS5B,CAAC;AAEX,eAAO,MAAM,2BAA2B;;;;;;;;;CAS9B,CAAC;AAEX,eAAO,MAAM,0BAA0B;;;;;;;;;CAS7B,CAAC;AAEX,eAAO,MAAM,+BAA+B;;;;;;;;;CAUlC,CAAC;AAEX,eAAO,MAAM,uBAAuB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAW1B,CAAC;AAEX,MAAM,MAAM,WAAW,GAAG,CAAC,OAAO,uBAAuB,CAAC,CAAC,MAAM,CAAC,CAAC;AAInE,MAAM,MAAM,cAAc,GAAG,mBAAmB,CAC9C,CAAC,OAAO,uBAAuB,CAAC,CAAC,MAAM,CAAC,EACxC,YAAY,GAAG,SAAS,CACzB,CAAC;AAEF,wBAAgB,gBAAgB,CAAC,KAAK,EAAE,OAAO,GAAG,KAAK,IAAI,cAAc,CAOxE\"}"},"dts":{"name":"/Users/edouardwautier/Projects/dust/types/node_modules/.cache/rollup-plugin-typescript2/placeholder/front/lib/assistant.d.ts","writeByteOrderMark":false,"text":"/**\n * Supported models\n */\nimport { ExtractSpecificKeys } from \"../../shared/typescipt_utils\";\nexport declare const GPT_4_MODEL_ID: \"gpt-4\";\nexport declare const GPT_4_TURBO_PREVIEW_MODEL_ID: \"gpt-4-turbo-preview\";\nexport declare const GPT_3_5_TURBO_MODEL_ID: \"gpt-3.5-turbo-1106\";\nexport declare const GPT_4_MODEL_CONFIG: {\n    providerId: \"openai\";\n    modelId: \"gpt-4\";\n    displayName: string;\n    contextSize: number;\n    recommendedTopK: number;\n    largeModel: boolean;\n    description: string;\n    shortDescription: string;\n};\nexport declare const GPT_4_TURBO_MODEL_CONFIG: {\n    readonly providerId: \"openai\";\n    readonly modelId: \"gpt-4-turbo-preview\";\n    readonly displayName: \"GPT 4\";\n    readonly contextSize: 128000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: true;\n    readonly description: \"OpenAI's most powerful and recent model (128k context).\";\n    readonly shortDescription: \"OpenAI's smartest model.\";\n};\nexport declare const GPT_3_5_TURBO_MODEL_CONFIG: {\n    readonly providerId: \"openai\";\n    readonly modelId: \"gpt-3.5-turbo-1106\";\n    readonly displayName: \"GPT 3.5 Turbo\";\n    readonly contextSize: 16384;\n    readonly recommendedTopK: 16;\n    readonly largeModel: false;\n    readonly description: \"OpenAI's cost-effective and high throughput model (16k context).\";\n    readonly shortDescription: \"OpenAI's fast model.\";\n};\nexport declare const CLAUDE_2_1_MODEL_ID: \"claude-2.1\";\nexport declare const CLAUDE_2_MODEL_ID: \"claude-2\";\nexport declare const CLAUDE_INSTANT_1_2_MODEL_ID: \"claude-instant-1.2\";\nexport declare const CLAUDE_DEFAULT_MODEL_CONFIG: {\n    readonly providerId: \"anthropic\";\n    readonly modelId: \"claude-2.1\";\n    readonly displayName: \"Claude 2.1\";\n    readonly contextSize: 200000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: true;\n    readonly description: \"Anthropic's superior performance model (200k context).\";\n    readonly shortDescription: \"Anthropic's smartest model.\";\n};\nexport declare const CLAUDE_INSTANT_DEFAULT_MODEL_CONFIG: {\n    readonly providerId: \"anthropic\";\n    readonly modelId: \"claude-instant-1.2\";\n    readonly displayName: \"Claude Instant 1.2\";\n    readonly contextSize: 100000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: false;\n    readonly description: \"Anthropic's low-latency and high throughput model (100k context)\";\n    readonly shortDescription: \"Anthropic's fast model.\";\n};\nexport declare const MISTRAL_LARGE_MODEL_ID: \"mistral-large-latest\";\nexport declare const MISTRAL_MEDIUM_MODEL_ID: \"mistral-medium\";\nexport declare const MISTRAL_NEXT_MODEL_ID: \"mistral-next\";\nexport declare const MISTRAL_SMALL_MODEL_ID: \"mistral-small\";\nexport declare const MISTRAL_LARGE_MODEL_CONFIG: {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-large-latest\";\n    readonly displayName: \"Mistral Large\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral latest `large` model (32k context).\";\n    readonly shortDescription: \"Mistral's large model.\";\n};\nexport declare const MISTRAL_NEXT_MODEL_CONFIG: {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-next\";\n    readonly displayName: \"Mistral (next)\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral's next generation model (32k context).\";\n    readonly shortDescription: \"Mistral's next model.\";\n};\nexport declare const MISTRAL_MEDIUM_MODEL_CONFIG: {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-medium\";\n    readonly displayName: \"Mistral Medium\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral latest `medium` model (32k context).\";\n    readonly shortDescription: \"Mistral's smartest model.\";\n};\nexport declare const MISTRAL_SMALL_MODEL_CONFIG: {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-small\";\n    readonly displayName: \"Mistral Small\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: false;\n    readonly description: \"Mistral latest model (8x7B Instruct, 32k context).\";\n    readonly shortDescription: \"Mistral's fast model.\";\n};\nexport declare const GEMINI_PRO_DEFAULT_MODEL_CONFIG: {\n    readonly providerId: \"google_vertex_ai\";\n    readonly modelId: \"gemini-pro\";\n    readonly displayName: \"Gemini Pro\";\n    readonly contextSize: 8192;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Google's best model for scaling across a wide range of tasks (8k context).\";\n    readonly shortDescription: \"Google's smartest model.\";\n};\nexport declare const SUPPORTED_MODEL_CONFIGS: readonly [{\n    readonly providerId: \"openai\";\n    readonly modelId: \"gpt-3.5-turbo-1106\";\n    readonly displayName: \"GPT 3.5 Turbo\";\n    readonly contextSize: 16384;\n    readonly recommendedTopK: 16;\n    readonly largeModel: false;\n    readonly description: \"OpenAI's cost-effective and high throughput model (16k context).\";\n    readonly shortDescription: \"OpenAI's fast model.\";\n}, {\n    providerId: \"openai\";\n    modelId: \"gpt-4\";\n    displayName: string;\n    contextSize: number;\n    recommendedTopK: number;\n    largeModel: boolean;\n    description: string;\n    shortDescription: string;\n}, {\n    readonly providerId: \"openai\";\n    readonly modelId: \"gpt-4-turbo-preview\";\n    readonly displayName: \"GPT 4\";\n    readonly contextSize: 128000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: true;\n    readonly description: \"OpenAI's most powerful and recent model (128k context).\";\n    readonly shortDescription: \"OpenAI's smartest model.\";\n}, {\n    readonly providerId: \"anthropic\";\n    readonly modelId: \"claude-2.1\";\n    readonly displayName: \"Claude 2.1\";\n    readonly contextSize: 200000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: true;\n    readonly description: \"Anthropic's superior performance model (200k context).\";\n    readonly shortDescription: \"Anthropic's smartest model.\";\n}, {\n    readonly providerId: \"anthropic\";\n    readonly modelId: \"claude-instant-1.2\";\n    readonly displayName: \"Claude Instant 1.2\";\n    readonly contextSize: 100000;\n    readonly recommendedTopK: 32;\n    readonly largeModel: false;\n    readonly description: \"Anthropic's low-latency and high throughput model (100k context)\";\n    readonly shortDescription: \"Anthropic's fast model.\";\n}, {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-large-latest\";\n    readonly displayName: \"Mistral Large\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral latest `large` model (32k context).\";\n    readonly shortDescription: \"Mistral's large model.\";\n}, {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-medium\";\n    readonly displayName: \"Mistral Medium\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral latest `medium` model (32k context).\";\n    readonly shortDescription: \"Mistral's smartest model.\";\n}, {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-next\";\n    readonly displayName: \"Mistral (next)\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Mistral's next generation model (32k context).\";\n    readonly shortDescription: \"Mistral's next model.\";\n}, {\n    readonly providerId: \"mistral\";\n    readonly modelId: \"mistral-small\";\n    readonly displayName: \"Mistral Small\";\n    readonly contextSize: 31500;\n    readonly recommendedTopK: 16;\n    readonly largeModel: false;\n    readonly description: \"Mistral latest model (8x7B Instruct, 32k context).\";\n    readonly shortDescription: \"Mistral's fast model.\";\n}, {\n    readonly providerId: \"google_vertex_ai\";\n    readonly modelId: \"gemini-pro\";\n    readonly displayName: \"Gemini Pro\";\n    readonly contextSize: 8192;\n    readonly recommendedTopK: 16;\n    readonly largeModel: true;\n    readonly description: \"Google's best model for scaling across a wide range of tasks (8k context).\";\n    readonly shortDescription: \"Google's smartest model.\";\n}];\nexport type ModelConfig = (typeof SUPPORTED_MODEL_CONFIGS)[number];\nexport type SupportedModel = ExtractSpecificKeys<(typeof SUPPORTED_MODEL_CONFIGS)[number], \"providerId\" | \"modelId\">;\nexport declare function isSupportedModel(model: unknown): model is SupportedModel;\n//# sourceMappingURL=assistant.d.ts.map"}}
